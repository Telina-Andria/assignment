{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab115d43",
   "metadata": {},
   "source": [
    "# Q1: What are missing values in a dataset? Why is it essential to handle missing values? Name some algorithms that are not affected by missing values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6737aac8",
   "metadata": {},
   "source": [
    "Missing data is defined as the values or data that is not stored (or not present) for some variable/s in the given dataset.\n",
    "Formally the missing values are categorized as follows: \n",
    "\n",
    "1. Missing Completely At Random (MCAR) :\n",
    "    In MCAR, the probability of data being missing is the same for all the observations. In this case, there is no relationship between the missing data and any other values observed or unobserved (the data which is not recorded) within the given dataset. That is, missing values are completely independent of other data. There is no pattern.\n",
    "\n",
    ".\n",
    "2. Missing At Random (MAR) :\n",
    "    MAR data means that the reason for missing values can be explained by variables on which you have complete information, as there is some relationship between the missing data and other values/data. In this case, the data is not missing for all the observations. It is missing only within sub-samples of the data, and there is some pattern in the missing values.\n",
    "    \n",
    "    For example, if you check the survey data, you may find that all the people have answered their ‘Gender,’ but ‘Age’ values are mostly missing for people who have answered their ‘Gender’ as ‘female.’ (The reason being most of the females don’t want to reveal their age.)\n",
    "    \n",
    ".\n",
    "3. Missing Not At Random (MNAR) :\n",
    "    Missing values depend on the unobserved data. If there is some structure/pattern in missing data and other observed data can not explain it, then it is considered to be Missing Not At Random (MNAR).\n",
    "\n",
    "    If the missing data does not fall under the MCAR or MAR, it can be categorized as MNAR. It can happen due to the reluctance of people to provide the required information. A specific group of respondents may not answer some questions in a survey.\n",
    "    \n",
    "It is important to handle the missing values appropriately because:\n",
    "* Many machine learning algorithms fail if the dataset contains missing values. However, algorithms like K-nearest and Naive Bayes support data with missing values.\n",
    "* You may end up building a biased machine learning model, leading to incorrect results.\n",
    "* Missing data can lead to a lack of precision in the statistical analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ae8dfd",
   "metadata": {},
   "source": [
    "# Q2: List down techniques used to handle missing data.  Give an example of each with python code.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f7ca30",
   "metadata": {},
   "source": [
    "Here are some common techniques for handling missing data using Python:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635685b0",
   "metadata": {},
   "source": [
    "Deleting the Missing Values: One way to handle missing data is to simply delete the rows or columns containing missing values. This technique can be useful when the amount of missing data is small. Here's an example of how to delete rows with missing values using pandas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6deba3e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Name   Age   Salary\n",
      "0  Alice  25.0  50000.0\n",
      "3  David  35.0  80000.0\n",
      "4    Eva  40.0  90000.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Creating a sample dataframe\n",
    "data = {'Name': ['Alice', 'Bob', 'Charlie', 'David', 'Eva'],\n",
    "        'Age': [25, 30, None, 35, 40],\n",
    "        'Salary': [50000, None, 70000, 80000, 90000]}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Dropping rows with missing values\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76657e5e",
   "metadata": {},
   "source": [
    "Imputing the Missing Values: Another way to handle missing data is to impute or fill in the missing values with some estimate. One common method is to use the mean or median value of the available data. Here's an example of how to impute missing values with the mean using pandas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b0857015",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Name   Age   Salary\n",
      "0    Alice  25.0  50000.0\n",
      "1      Bob  30.0  72500.0\n",
      "2  Charlie  32.5  70000.0\n",
      "3    David  35.0  80000.0\n",
      "4      Eva  40.0  90000.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Creating a sample dataframe\n",
    "data = {'Name': ['Alice', 'Bob', 'Charlie', 'David', 'Eva'],\n",
    "        'Age': [25, 30, None, 35, 40],\n",
    "        'Salary': [50000, None, 70000, 80000, 90000]}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Filling in missing values with the mean for Age and Salary columns\n",
    "df['Age'].fillna(df['Age'].mean(), inplace=True)\n",
    "df['Salary'].fillna(df['Salary'].mean(), inplace=True)\n",
    "\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83972e6a",
   "metadata": {},
   "source": [
    "Imputing the Missing Values for Categorical Features: If the missing data is categorical, we can impute it with the mode (most frequent value). Here's an example of how to impute missing categorical values with the mode using pandas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "11ef7732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Name Gender   Salary\n",
      "0    Alice      F  50000.0\n",
      "1      Bob      M      NaN\n",
      "2  Charlie      M  70000.0\n",
      "3    David      M  80000.0\n",
      "4      Eva      M  90000.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Creating a sample dataframe\n",
    "data = {'Name': ['Alice', 'Bob', 'Charlie', 'David', 'Eva'],\n",
    "        'Gender': ['F', None, 'M', 'M', None],\n",
    "        'Salary': [50000, None, 70000, 80000, 90000]}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Filling in missing categorical values with the mode\n",
    "df['Gender'].fillna(df['Gender'].mode()[0], inplace=True)\n",
    "\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0fe3247",
   "metadata": {},
   "source": [
    "Imputing the Missing Values using Sci-kit Learn Library: Sci-kit Learn library has a SimpleImputer class that can be used to impute missing values with various strategies such as mean, median or most frequent value (mode). Here's an example of how to use SimpleImputer to impute missing values with mean using scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a0b0a5eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Age   Salary     Name\n",
      "0  25.0  50000.0    Alice\n",
      "1  30.0  72500.0      Bob\n",
      "2  32.5  70000.0  Charlie\n",
      "3  35.0  80000.0    David\n",
      "4  40.0  90000.0      Eva\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Creating a sample dataframe\n",
    "data = {'Name': ['Alice', 'Bob', 'Charlie', 'David', 'Eva'],\n",
    "        'Age': [25, 30, None, 35, 40],\n",
    "        'Salary': [50000, None, 70000, 80000, 90000]}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Creating an instance of SimpleImputer to impute missing values with mean\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "\n",
    "# Fitting and transforming the dataframe\n",
    "df_imputed = pd.DataFrame(imputer.fit_transform(df[['Age', 'Salary']]), columns=['Age', 'Salary'])\n",
    "\n",
    "# Concatenating the Name column with the imputed dataframe\n",
    "df_imputed['Name'] = df['Name']\n",
    "\n",
    "print(df_imputed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe72477a",
   "metadata": {},
   "source": [
    "Using “Missingness” as a Feature: Another approach is to treat missing values as a separate category and create a new feature indicating whether a value is missing or not. Here's an example of how to create a new feature indicating whether a value is missing or not using pandas:\n",
    "python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "770ad8c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Name   Age   Salary  Age_Missing  Salary_Missing\n",
      "0    Alice  25.0  50000.0            0               0\n",
      "1      Bob  30.0      NaN            0               1\n",
      "2  Charlie   NaN  70000.0            1               0\n",
      "3    David  35.0  80000.0            0               0\n",
      "4      Eva  40.0  90000.0            0               0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Creating a sample dataframe\n",
    "data = {'Name': ['Alice', 'Bob', 'Charlie', 'David', 'Eva'],\n",
    "        'Age': [25, 30, None, 35, 40],\n",
    "        'Salary': [50000, None, 70000, 80000, 90000]}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Creating a new feature indicating whether a value is missing or not\n",
    "df['Age_Missing'] = df['Age'].isnull().astype(int)\n",
    "df['Salary_Missing'] = df['Salary'].isnull().astype(int)\n",
    "\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49cd7ee5",
   "metadata": {},
   "source": [
    "# Q3: Explain the imbalanced data. What will happen if imbalanced data is not handled?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab187639",
   "metadata": {},
   "source": [
    "Data imbalance usually reflects an unequal distribution of classes within a dataset. For example, in a credit card fraud detection dataset, most of the credit card transactions are not fraud and a very few classes are fraud transactions. \n",
    "\n",
    "If imbalanced data is not handled, it can lead to biased models and poor performance. In a classification problem with imbalanced data, the model may be biased towards the majority class and may fail to accurately predict the minority class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c044054b",
   "metadata": {},
   "source": [
    "# Q4: What are Up-sampling and Down-sampling? Explain with an example when up-sampling and downsampling are required.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252b1592",
   "metadata": {},
   "source": [
    "When we are using an imbalanced dataset, we can oversample the minority class using replacement. This technique is called Up-sampling. Similarly, we can randomly delete rows from the majority class to match them with the minority class which is called Down-sampling. After sampling the data we can get a balanced dataset for both majority and minority classes.\n",
    "\n",
    "Example where up-sampling is required is if we have a low-resolution image and we want to increase its size without losing quality, we can up-sample it using interpolation techniques such as bilinear or bicubic interpolation.\n",
    "\n",
    "Example where down-sampling is required is  if we have a digital audio signal sampled at 44.1 kHz and we want to reduce its size to save storage space or reduce computational costs, we can downsample it to 22.05 kHz or lower."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302c0e6f",
   "metadata": {},
   "source": [
    "# Q5: What is data Augmentation? Explain SMOTE.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "843050c2",
   "metadata": {},
   "source": [
    "Data augmentation is a technique used to increase the size and diversity of a dataset by creating new samples from the existing ones. This is useful when the size of the dataset is small or imbalanced.\n",
    "\n",
    "Synthetic Minority Oversampling Technique or SMOTE is another technique to oversample the minority class. Here's how SMOTE works:\n",
    "1. For each sample in the minority class, SMOTE selects k nearest neighbors from the same class.\n",
    "2. SMOTE then generates new samples by interpolating between the selected sample and its k nearest neighbors.\n",
    "3. The amount of interpolation is controlled by a parameter called the sampling ratio, which determines the number of synthetic samples to be generated.\n",
    "\n",
    "\n",
    "<img src=\"img/smote.webp\" width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9643da9",
   "metadata": {},
   "source": [
    "# Q6: What are outliers in a dataset? Why is it essential to handle outliers?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adfce2d5",
   "metadata": {},
   "source": [
    "Outlier is an observation in a given dataset that lies far from the rest of the observations. That means an outlier is vastly larger or smaller than the remaining values in the set. An outlier may occur due to the variability in the data, or due to experimental error/human error.\n",
    "\n",
    "Handling outliers is essential because they can significantly affect the statistical analysis of a dataset. If outliers are not handled properly, they can skew the results of statistical analysis, leading to inaccurate conclusions and decisions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1606376e",
   "metadata": {},
   "source": [
    "# Q7: You are working on a project that requires analyzing customer data. However, you notice that some of the data is missing. What are some techniques you can use to handle the missing data in your analysis?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96dc73b",
   "metadata": {},
   "source": [
    "There are several techniques that can be used to handle missing data in analysis:\n",
    "\n",
    "1. **Deletion**: You can delete the rows or columns that contain missing data. However, this technique can lead to a loss of information and reduce the sample size.\n",
    "\n",
    "2. **Imputation**: You can use imputation techniques to replace missing data with estimated values. Common imputation techniques include mean imputation, median imputation, and regression imputation.\n",
    "\n",
    "3. **Model-based methods**: You can use statistical models to estimate missing values based on other variables in the dataset.\n",
    "\n",
    "4. **Multiple imputation**: This technique involves creating multiple imputed datasets and analyzing them separately to obtain a final estimate.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0853cfa8",
   "metadata": {},
   "source": [
    "# Q8: You are working with a large dataset and find that a small percentage of the data is missing. What are some strategies you can use to determine if the missing data is missing at random or if there is a pattern to the missing data?\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaec0c3f",
   "metadata": {},
   "source": [
    "* **Explore Data Patterns**: Start by visually inspecting your data. Create summary statistics, histograms, or other visualization techniques to look for patterns or trends in the missing data. You might notice if certain groups or variables have more missing values.\n",
    "\n",
    "* **Missing Data Heatmap**: Create a heatmap or matrix plot to visualize missing data patterns. This can help you see if there are clusters or patterns in the missingness. \n",
    "\n",
    "* **Correlation Analysis**: Calculate correlations between missingness and other variables. If there is a correlation between missingness and certain variables, it could suggest non-random missingness. Tools like the chi-squared test can help assess this.\n",
    "\n",
    "* **Compare Missing and Complete Data**: Compare the characteristics of cases with missing data to those with complete data. You can use summary statistics, t-tests, or other statistical tests to see if there are significant differences between these groups. Significant differences might indicate non-random missingness.\n",
    "\n",
    "* **Domain Knowledge**: Consult subject matter experts who have a deep understanding of the data and the context. They may have insights into why data is missing and whether it's likely to be MAR.\n",
    "\n",
    "* **Multiple Imputation**: Use multiple imputation techniques to estimate missing values and compare the results with the observed data. If the imputed values match the observed data closely, it suggests that the data may be MAR. Multiple imputation assumes that missing data are missing at random.\n",
    " \n",
    "Determining the nature of missing data is not always straightforward and may require a combination of these strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea114083",
   "metadata": {},
   "source": [
    "# Q9: Suppose you are working on a medical diagnosis project and find that the majority of patients in the dataset do not have the condition of interest, while a small percentage do. What are some strategies you can use to evaluate the performance of your machine learning model on this imbalanced dataset?\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5024675f",
   "metadata": {},
   "source": [
    "* **Resampling techniques**: You can use resampling techniques, such as oversampling or undersampling, to balance the dataset before training your model\n",
    "\n",
    "* **Confusion matrix**: You can use a confusion matrix to evaluate the performance of your model. This will give you a breakdown of the true positives, true negatives, false positives, and false negatives.\n",
    "\n",
    "* **Class weighting**: You can assign different weights to different classes to balance the importance of each class in the loss function.\n",
    "\n",
    "* **Precision and recall**: Precision and recall are two commonly used metrics for evaluating performance on imbalanced datasets. Precision measures the proportion of true positives among all positive predictions, while recall measures the proportion of true positives among all actual positives.\n",
    "\n",
    "* **F1 score**: The F1 score is the harmonic mean of precision and recall, and can be a useful metric for evaluating performance on imbalanced datasets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52300a88",
   "metadata": {},
   "source": [
    "# Q10: When attempting to estimate customer satisfaction for a project, you discover that the dataset is unbalanced, with the bulk of customers reporting being satisfied. What methods can you employ to balance the dataset and down-sample the majority class? \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45457da3",
   "metadata": {},
   "source": [
    "To balance the dataset and down-sample the majority class, you can use the following methods:\n",
    "\n",
    "* **Random under-sampling**: randomly select a subset of the majority class to match the size of the minority class.\n",
    "\n",
    "* **Cluster-based under-sampling**: use clustering techniques to identify clusters of data points in the majority class and select representative samples from each cluster.\n",
    "\n",
    "* **Tomek Links**: identify pairs of data points that are nearest neighbors but belong to different classes, and remove the majority class data points in these pairs.\n",
    "\n",
    "* **Edited Nearest Neighbors**: remove data points in the majority class that are misclassified by their k-nearest neighbors in the minority class.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c68710",
   "metadata": {},
   "source": [
    "# Q11: You discover that the dataset is unbalanced with a low percentage of occurrences while working on a project that requires you to estimate the occurrence of a rare event. What methods can you employ to balance the dataset and up-sample the minority class? \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e7cae9",
   "metadata": {},
   "source": [
    "There are several methods you can use to balance an unbalanced dataset and up-sample the minority class. One approach is to use oversampling techniques such as:\n",
    "\n",
    "* **Random oversampling**: randomly duplicating samples from the minority class to increase its frequency\n",
    "\n",
    "* **SMOTE (Synthetic Minority Over-sampling Technique)**: generating new synthetic samples by interpolating between existing ones in the minority class\n",
    "\n",
    "* **ADASYN (Adaptive Synthetic Sampling)**: similar to SMOTE but generates more synthetic samples in regions of the feature space where the density of the minority class is lower\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
