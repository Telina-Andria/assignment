{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b6693b1",
   "metadata": {},
   "source": [
    "# Q1: Define overfitting and underfitting in machine learning. What are the consequences of each, and how can they be mitigated?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b7d2d4",
   "metadata": {},
   "source": [
    "1. Overfitting occurs when a machine learning model has a high accuracy in training data, but performs poorly on new, unseen test data.\n",
    "\n",
    "    Overfitting can lead to poor performance on new, unseen data because the model is too specialized to the training data and is not able to generalize well. This can result in a model that is not useful in real-world scenarios. To mitigate overfitting, you can use techniques such as regularization, early stopping, and data augmentation.\n",
    ".\n",
    "\n",
    "2. Underfitting, on the other hand, occurs when the model is too simple and has a low accuracy on both the training and test data. \n",
    "\n",
    "    Underfitting can result in a model that is too simple and unable to capture the underlying patterns in the data. This can lead to poor performance on both the training and test data. To mitigate underfitting, you can use techniques such as : adding more features or layers , using a more powerful algorithm, increasing the amount of training data, improving the quality of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022ff6a6",
   "metadata": {},
   "source": [
    "\n",
    "# Q2: How can we reduce overfitting? Explain in brief"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebfcd9a1",
   "metadata": {},
   "source": [
    "To reduce overfitting, you can consider the following techniques:\n",
    "\n",
    "* Feature selection: Select only the most relevant features for training the model.\n",
    "* Regularization adds a penalty term to the loss function to prevent the model from becoming too complex.\n",
    "* Cross-validation: Instead of relying solely on a single train-test split, you can use techniques like k-fold cross-validation. \n",
    "* Early stopping stops training when the performance on a validation set stops improving, preventing the model from overfitting to the training data. \n",
    "* Data augmentation involves creating new training examples by applying transformations to the existing data, which can help the model learn to generalize better.\n",
    "* Dropout: Dropout is a technique where randomly selected neurons or connections are temporarily dropped during training.\n",
    "* Ensemble methods: Combine multiple models to make predictions. Ensemble methods, such as bagging or boosting, can help reduce overfitting by aggregating the predictions of multiple models trained on different subsets of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff8ab96",
   "metadata": {},
   "source": [
    "# Q3: Explain underfitting. List scenarios where underfitting can occur in ML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc938e45",
   "metadata": {},
   "source": [
    "Underfitting occurs when a machine learning model is too simple to capture the underlying patterns in the data. \n",
    "\n",
    "Some scenarios where underfitting can occur in machine learning include:\n",
    "* Insufficient data: When there is not enough data to train a complex model, the model may not be able to capture the underlying patterns in the data and may underfit.\n",
    "* Insufficient features: When the model does not have enough features to represent the complexity of the data, it may not be able to capture the underlying patterns and may underfit.\n",
    "* Over-regularization: When the regularization parameter is set too high.\n",
    "* Using a simple model: When a simple model, such as linear regression, is used to model complex non-linear relationships in the data, it may underfit.\n",
    "* Using a high bias algorithm: Algorithms with high bias, such as decision trees with low depth, can be too simple to capture the underlying patterns in the data and may underfit.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce1bbbbe",
   "metadata": {},
   "source": [
    "# Q4: Explain the bias-variance tradeoff in machine learning. What is the relationship between bias and variance, and how do they affect model performance?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c712c030",
   "metadata": {},
   "source": [
    "If our model is too simple and has very few parameters then it may have high bias and low variance. On the other hand if our model has large number of parameters then it’s going to have high variance and low bias. So we need to find the right/good balance without overfitting and underfitting the data.\n",
    "\n",
    "This tradeoff in complexity is why there is a tradeoff between bias and variance. An algorithm can’t be more complex and less complex at the same time.\n",
    "\n",
    "To build a good model, we need to find a good balance between bias and variance such that it minimizes the total error. Mathematicaly: \n",
    "\n",
    "Let the variable we are trying to predict as Y and other covariates as X. We assume there is a relationship between the two such that\n",
    "\n",
    "Y=f(X) + e\n",
    "\n",
    "Where e is the error term and it’s normally distributed with a mean of 0.\n",
    "\n",
    "We will make a model f^(X) of f(X) using linear regression or any other modeling technique.\n",
    "\n",
    "So the expected squared error at a point x is\n",
    "<img src=\"img/err.webp\">\n",
    "\n",
    "The Err(x) can be further decomposed as\n",
    "<img src=\"img/err2.webp\">\n",
    "\n",
    "Err(x) is the sum of Bias², variance and the irreducible error.\n",
    "\n",
    "Irreducible error is the error that can’t be reduced by creating good models. It is a measure of the amount of noise in our data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bda0283",
   "metadata": {},
   "source": [
    "# Q5: Discuss some common methods for detecting overfitting and underfitting in machine learning models. How can you determine whether your model is overfitting or underfitting?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e58a51",
   "metadata": {},
   "source": [
    "There are several methods for detecting overfitting and underfitting in machine learning models. Here are some common methods:\n",
    "\n",
    "    1. Visual inspection: One of the simplest ways to detect overfitting or underfitting is to plot the training and validation loss or accuracy curves against the number of training epochs. If the training loss continues to decrease while the validation loss starts to increase, it is a sign of overfitting. If both the training and validation losses remain high, it is a sign of underfitting.\n",
    "\n",
    "    2. Cross-validation: Cross-validation is a technique where the data is split into multiple subsets and the model is trained and evaluated on different combinations of these subsets. If the model performs well on all the subsets, it is a sign that the model is not overfitting. If the model performs poorly on all the subsets, it is a sign that the model is underfitting.\n",
    "\n",
    "    3. Evaluation metrics: Evaluation metrics such as accuracy, precision, recall, and F1 score can be used to measure the performance of the model on both the training and test data. If the model performs well on the training data but poorly on the test data, it is a sign of overfitting. If the model performs poorly on both the training and test data, it is a sign of underfitting.\n",
    "\n",
    "    4. Regularization: Regularization techniques such as L1 or L2 regularization can be used to add a penalty term to the loss function during training to prevent overfitting. If the regularization parameter is set too high, it can result in underfitting.\n",
    "\n",
    "    5. Learning curves: Learning curves can be plotted to show how the performance of the model changes as the amount of training data increases. If the model's performance improves with more data, it is a sign that the model is not underfitting. If the performance plateaus, it is a sign that the model may be underfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a7f416",
   "metadata": {},
   "source": [
    "# Q6: Compare and contrast bias and variance in machine learning. What are some examples of high bias and high variance models, and how do they differ in terms of their performance?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6192de29",
   "metadata": {},
   "source": [
    "|Aspect|Bias|Variance|\n",
    "| --- | --- | --- |\n",
    "|Definition|Error due to oversimplified model|Error due to sensitivity to training data noise|\n",
    "|Impact on Model Performance |Poor performance on both training and testing data\t|Good performance on training data but poor testing data performance|\n",
    "|Sources\t|Simple model, incorrect assumptions, omitted features\t|Complex model, insufficient data|\n",
    "|Techniques to Address\t|Use more complex models, increase features, apply regularization\t|Use simpler models, increase training data, use cross-validation, employ ensemble methods like bagging and boosting|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb97bf82",
   "metadata": {},
   "source": [
    "Example of high bias model is a linear regression model used to predict a non-linear relationship between variables, such as trying to fit a sine wave with a straight line. High bias models tend to have similar performance on both training and testing data, but that performance is generally poor. They systematically underpredict or overpredict the target variable.\n",
    "\n",
    "Example of high varianc model is  a decision tree with a large depth or a complex neural network with too many layers and parameters.High variance models tend to have a large gap between their training and testing performance. While they may achieve high accuracy on the training data, they fail to generalize to new, unseen data, making them unreliable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d84a10",
   "metadata": {},
   "source": [
    "# Q7: What is regularization in machine learning, and how can it be used to prevent overfitting? Describe some common regularization techniques and how they work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebaf6aa2",
   "metadata": {},
   "source": [
    "Regularization is a technique used in machine learning to prevent overfitting and improve the generalization of models. Overfitting occurs when a model fits the training data too closely, capturing noise and idiosyncrasies in the data, which can lead to poor performance on unseen data. Regularization introduces a penalty term to the model's cost function, discouraging it from becoming too complex or fitting the noise in the training data.\n",
    "\n",
    "There are two common types of regularization techniques in machine learning:\n",
    "\n",
    "* L1 Regularization (Lasso):\n",
    "\n",
    "    L1 regularization adds a penalty term to the cost function that is proportional to the absolute values of the model's coefficients.\n",
    "    It encourages sparsity in the model by making some of the coefficients exactly zero, effectively selecting a subset of the most important features.\n",
    "    L1 regularization is useful for feature selection and can simplify the model by eliminating irrelevant features.\n",
    " \n",
    ".   \n",
    "* L2 Regularization (Ridge):\n",
    "\n",
    "    L2 regularization adds a penalty term to the cost function that is proportional to the square of the model's coefficients.\n",
    "    It discourages the model from having very large coefficients, which can help in reducing the model's sensitivity to the training data's noise.\n",
    "    L2 regularization is particularly useful when there are many correlated features, as it tends to distribute the weights more evenly among them."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
