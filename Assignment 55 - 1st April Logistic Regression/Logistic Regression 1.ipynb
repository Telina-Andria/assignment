{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "885c3bcf",
   "metadata": {},
   "source": [
    "# Q1. Explain the difference between linear regression and logistic regression models. Provide an example of a scenario where logistic regression would be more appropriate.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a4ecad",
   "metadata": {},
   "source": [
    "In the **linear regression** model, the dependent variable is numeric and **continuous**, meaning that the predicted value can take on an infinite number of possibilities. It assumes that there is a **linear relationship** between the independent and dependent variables.\n",
    "\n",
    "On the other hand, in the **logistic regression** model, the dependent variable is **categorical**, which means that we are trying to predict the class of the input data. The output of a logistic regression model is a **probability**, which can be used to classify the data into two or more categories. It assumes that there is a **non-linear relationship** between the independent and dependent variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f0a2425",
   "metadata": {},
   "source": [
    "# Q2. What is the cost function used in logistic regression, and how is it optimized?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed55f8d",
   "metadata": {},
   "source": [
    "The cost function used in logistic regression is the cross-entropy loss, also known as the log loss.\n",
    "\n",
    "The cross-entropy loss is defined as follows:\n",
    "\n",
    "$\n",
    "J(\\theta) = -\\sum_{i=1}^n y_i \\log(h_\\theta(x_i)) + (1-y_i) \\log(1-h_\\theta(x_i))\n",
    "$\n",
    "\n",
    "where:\n",
    "* y is the actual label (0 or 1)\n",
    "* hθ(x) is the predicted probability of the positive class\n",
    "* θ is the vector of model parameters\n",
    "\n",
    "The log loss is a convex function, which means that it has a single global minimum. This makes it easier to optimize using gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e387832",
   "metadata": {},
   "source": [
    "# Q3. Explain the concept of regularization in logistic regression and how it helps prevent overfitting.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1704d5b1",
   "metadata": {},
   "source": [
    "Regularization addresses overfitting by adding a penalty term to the cost function. This penalty term penalizes large model parameters, effectively shrinking them towards zero. By reducing the magnitude of the parameters, the model becomes less sensitive to the specific details of the training data and more likely to capture the underlying patterns that generalize well to new data.\n",
    "\n",
    "There are three common regularization techniques used in logistic regression: Lasso, Ridge, and Elastic Net.\n",
    "\n",
    "L1 Regularization:\n",
    "\n",
    "$\n",
    "J(\\theta) = -\\sum_{i=1}^n y_i \\log(h_\\theta(x_i)) + (1-y_i) \\log(1-h_\\theta(x_i)) + \\lambda |\\theta|_1\n",
    "$\n",
    "\n",
    "L2 Regularization:\n",
    "\n",
    "$\n",
    "J(\\theta) = -\\sum_{i=1}^n y_i \\log(h_\\theta(x_i)) + (1-y_i) \\log(1-h_\\theta(x_i)) + \\lambda |\\theta|_2^2\n",
    "$\n",
    "\n",
    "Elastic Net Regularization:\n",
    "\n",
    "$\n",
    "J(\\theta) = -\\sum_{i=1}^n y_i \\log(h_\\theta(x_i)) + (1-y_i) \\log(1-h_\\theta(x_i)) + \\lambda_1 |\\theta|_1 + \\lambda_2 |\\theta|_2^2\n",
    "$\n",
    "\n",
    "where:\n",
    "\n",
    "* n : is the number of data points\n",
    "* $y_i$ is the actual label for the $i$th data point (0 or 1)\n",
    "* $h_θ$($x_i$) is the predicted probability of the positive class for the $i$th data point\n",
    "* θ is the vector of model parameters\n",
    "* λ is the regularization parameter for L1 or L2 regularization\n",
    "* $λ_1$ is the regularization parameter for L1 regularization in Elastic Net\n",
    "* $λ_2$ is the regularization parameter for L2 regularization in Elastic Net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b1d20d",
   "metadata": {},
   "source": [
    "# Q4. What is the ROC curve, and how is it used to evaluate the performance of the logistic regression model?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e239c19",
   "metadata": {},
   "source": [
    "The ROC curve is a graphical plot that illustrates the performance of a binary classifier system as its discrimination threshold is varied.\n",
    "\n",
    "The ROC curve plots two important metrics:\n",
    "\n",
    "* True Positive Rate (TPR): The proportion of actual positive cases that are correctly identified as positive. It measures the model's ability to correctly identify the positive class.\n",
    "\n",
    "* False Positive Rate (FPR): The proportion of actual negative cases that are incorrectly identified as positive. It measures the model's tendency to falsely classify negative instances as positive.\n",
    "\n",
    "\n",
    "The ROC curve is created by plotting the TPR against the FPR at various threshold values.\n",
    "\n",
    "A perfect classifier would have a ROC curve that passes through the upper left corner (TPR = 1, FPR = 0), indicating that it can perfectly distinguish between positive and negative cases. A random classifier would have a ROC curve that lies along the diagonal line (TPR = FPR), indicating that it performs no better than random guessing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00686f47",
   "metadata": {},
   "source": [
    "# Q5. What are some common techniques for feature selection in logistic regression? How do these techniques help improve the model's performance?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0a78b8",
   "metadata": {},
   "source": [
    "Here are some common techniques for feature selection in logistic regression:\n",
    "\n",
    "* **Univariate Feature Selection**: This method evaluates each feature individually based on its statistical relationship with the target variable. Common univariate methods include chi-square test, ANOVA F-test, and correlation coefficient.\n",
    "\n",
    "* **Recursive Feature Elimination (RFE)**: RFE starts with all features and iteratively removes the least important feature based on a ranking criterion, such as the model's performance or feature importance scores.\n",
    "\n",
    "* **L1 Regularization (Lasso Regression)**: L1 regularization penalizes the absolute values of the model coefficients, effectively shrinking some coefficients to zero. Features with zero coefficients are considered irrelevant and can be removed.\n",
    "\n",
    "* **Tree-based Feature Selection**: Decision trees and random forests can be used to assess feature importance based on their contribution to splitting criteria. Features with low importance scores can be eliminated.\n",
    "\n",
    "* **Correlation-based Feature Selection**: Features that are highly correlated with each other can be redundant and may introduce multicollinearity issues. Correlation analysis can identify and remove highly correlated features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e81a8f30",
   "metadata": {},
   "source": [
    "# Q6. How can you handle imbalanced datasets in logistic regression? What are some strategies for dealing with class imbalance?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98fce5a0",
   "metadata": {},
   "source": [
    " To address this issue, several strategies can be employed:\n",
    "\n",
    "* **Data Resampling**: This involves adjusting the class distribution by either oversampling the minority class or undersampling the majority class. \n",
    "\n",
    "* **Cost-Sensitive Learning**: This approach assigns different misclassification costs to different classes, penalizing errors on the minority class more heavily. This forces the model to pay more attention to the minority class and improve its prediction accuracy.\n",
    "\n",
    "* **Synthetic Minority Over-sampling Technique (SMOTE)**: SMOTE generates new synthetic minority class instances by interpolating between existing minority class instances. \n",
    "\n",
    "* **Ensemble Methods**: Ensemble methods like Random Forest or AdaBoost can be used to combine multiple weak learners into a strong learner. These methods can be less sensitive to class imbalance and provide better overall performance.\n",
    "\n",
    "* **Evaluation Metrics**: When evaluating model performance, consider metrics that are less sensitive to class imbalance, such as precision, recall, and F1-score, instead of relying solely on accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f08e63",
   "metadata": {},
   "source": [
    "# Q7. Can you discuss some common issues and challenges that may arise when implementing logistic regression, and how they can be addressed? For example, what can be done if there is multicollinearity among the independent variables?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc32b8bd",
   "metadata": {},
   "source": [
    "When dealing with multicollinearity we can:\n",
    "\n",
    "* **Remove highly correlated features**: Identify and remove features that are highly correlated with each other, keeping only the most informative ones.\n",
    "\n",
    "* **Principal Component Analysis (PCA)**: Use PCA to transform the original features into a smaller set of uncorrelated principal components, reducing multicollinearity.\n",
    "\n",
    "Another problem can be Overfitting. So in order to address this issue :\n",
    "\n",
    "* **Regularization**: Apply regularization techniques like Ridge, Lasso, or Elastic Net to penalize large model coefficients, effectively shrinking them towards zero. This reduces the model's complexity and makes it less prone to overfitting.\n",
    "\n",
    "* **Cross-validation**: Use cross-validation techniques like k-fold cross-validation to evaluate the model's performance on different subsets of the training data. This helps identify overfitting and select the best hyperparameters.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
