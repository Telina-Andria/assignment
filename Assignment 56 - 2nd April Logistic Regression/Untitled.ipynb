{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d1b1e5e",
   "metadata": {},
   "source": [
    "# Q1. What is the purpose of grid search cv in machine learning, and how does it work?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3736f438",
   "metadata": {},
   "source": [
    "In every machine learning model, there are numerous parameters that can be adjusted. With so many potential combinations of parameters, it can be challenging to determine which ones will most effectively suit our dataset. To address this issue, we can use grid search CV to try out various combinations of parameters and identify the optimal configuration for our model.\n",
    "\n",
    "Grid search CV works by creating a grid of all possible combinations of hyperparameter values that we want to test. Then, for each combination of hyperparameters, we train a new model and evaluate its performance using cross-validation.\n",
    "\n",
    "Once we have evaluated all possible combinations of hyperparameters, we can choose the combination that produced the best performance on our dataset. This allows us to fine-tune our model and improve its accuracy on new, unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff9fe34",
   "metadata": {},
   "source": [
    "# Q2. Describe the difference between grid search cv and randomize search cv, and when might you choose one over the other?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c17f9ee1",
   "metadata": {},
   "source": [
    "Grid search CV  tries every possible combination of hyperparameter values specified by the user. This can be computationally expensive. \n",
    "\n",
    "Randomized search CV is a hyperparameter optimization technique that samples a random subset of hyperparameter values from a specified distribution.This is less computationally expensive than grid search CV.\n",
    "\n",
    "When to choose grid search CV:\n",
    "* When you have a small number of hyperparameters to tune.\n",
    "* When you have the computational resources to run grid search CV.\n",
    "\n",
    "When to choose randomized search CV:\n",
    "* When you have a large number of hyperparameters to tune.\n",
    "* When you are limited on computational resources."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e837fa",
   "metadata": {},
   "source": [
    "# Q3. What is data leakage, and why is it a problem in machine learning? Provide an example.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6107c101",
   "metadata": {},
   "source": [
    "Data leakage happens when your training data contains information about the target, but similar data will not be available when the model is used for prediction. This leads to high performance on the training set (and possibly even the validation data), but the model will perform poorly in production.\n",
    "\n",
    "There are two main types of leakage: target leakage and train-test contamination.\n",
    " \n",
    "**Target leakage** : occurs when your predictors include data that will not be available at the time you make predictions. It is important to think about target leakage in terms of the timing or chronological order that data becomes available, not merely whether a feature helps make good predictions.\n",
    "     \n",
    "     For example, in predicting pneumonia, if the feature indicating whether a person took antibiotics is included in the model, it can introduce data leakage. This is because antibiotics are typically taken after being diagnosed with pneumonia, making this feature dependent on the target variable.\n",
    "     \n",
    "**Train-Test Contamination** :\n",
    "A different type of leak occurs when you aren't careful to distinguish training data from validation data.\n",
    "\n",
    "Recall that validation is meant to be a measure of how the model does on data that it hasn't considered before. You can corrupt this process in subtle ways if the validation data affects the preprocessing behavior. This is sometimes called train-test contamination.\n",
    "\n",
    "    For example, imagine you run preprocessing (like fitting an imputer for missing values) before calling train_test_split(). The end result? Your model may get good validation scores, giving you great confidence in it, but perform poorly when you deploy it to make decisions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e3d69c",
   "metadata": {},
   "source": [
    "# Q4. How can you prevent data leakage when building a machine learning model?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0df32e9",
   "metadata": {},
   "source": [
    "There are a number of things you can do to prevent data leakage when building a machine learning model, including:\n",
    "\n",
    "* **Understand your data and the machine learning task**. This will help you to identify any potential sources of data leakage.\n",
    "* **Split your data into training and testing sets before performing any preprocessing or feature engineering**. This will ensure that the model is not trained on information that will not be available at prediction time.\n",
    "* **Use cross-validation to evaluate your model**. This will help to identify any data leakage that may be occurring.\n",
    "* **Be careful when using external data**. Make sure that the external data is relevant to the machine learning task and that it does not contain any information that will not be available at prediction time.\n",
    "* **Use a holdout dataset to evaluate the final model**. This will help to ensure that the model is not overfitting to the training data.\n",
    "* **Be careful when using features that are encoded using timestamps or other sequential identifiers**.\n",
    "\n",
    "\n",
    "To use make_pipeline to prevent data leakage when building a machine learning model, you can follow these steps:\n",
    "\n",
    "1. Create a pipeline object using make_pipeline(). This will take a list of estimators as input, where each estimator represents a step in the preprocessing or machine learning pipeline.\n",
    "2. Add the preprocessing steps to the pipeline object. Make sure that these steps do not leak any information about the target variable.\n",
    "3. Add the machine learning estimator to the pipeline object.\n",
    "4. Fit the pipeline object to the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad3307b",
   "metadata": {},
   "source": [
    "# Q5. What is a confusion matrix, and what does it tell you about the performance of a classification model?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb2661f",
   "metadata": {},
   "source": [
    "A confusion matrix is a table that summarizes the performance of a classification model. It shows the number of correct and incorrect predictions for each class in the model.\n",
    "\n",
    "The confusion matrix has four main categories:\n",
    "\n",
    "    True positives (TP): These are the examples that the model correctly predicted as belonging to a particular class.\n",
    "    False positives (FP): These are the examples that the model incorrectly predicted as belonging to a particular class.\n",
    "    True negatives (TN): These are the examples that the model correctly predicted as not belonging to a particular class.\n",
    "    False negatives (FN): These are the examples that the model incorrectly predicted as not belonging to a particular class.\n",
    "\n",
    "The confusion matrix can be used to calculate a number of performance metrics, such as accuracy, precision, recall, and F1 score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36bf4e62",
   "metadata": {},
   "source": [
    "# Q6. Explain the difference between precision and recall in the context of a confusion matrix.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bbebc09",
   "metadata": {},
   "source": [
    "Precision is the proportion of positive predictions that are correct. It is calculated as follows:\n",
    "\n",
    "    Precision = TP / (TP + FP)\n",
    "    . Precision is a metric that is used when the cost of false positives is high. In other words, precision is important when we want to avoid making incorrect positive predictions.\n",
    "    \n",
    "Recall is the proportion of all positive examples that are correctly predicted. It is calculated as follows:\n",
    "\n",
    "    Recall = TP / (TP + FN)\n",
    "    . Recall is a metric that is used when the cost of false negatives is high. In other words, recall is important when we want to avoid missing positive cases.\n",
    "    \n",
    "In other words, precision measures how well the model can identify true positives, while recall measures how well the model can avoid false negatives."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8e6fc2",
   "metadata": {},
   "source": [
    "# Q7. How can you interpret a confusion matrix to determine which types of errors your model is making?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e3af75",
   "metadata": {},
   "source": [
    "A confusion matrix is a table that compares the predicted and actual values of a classification model. It consists of four cells: true positives (TP), false positives (FP), false negatives (FN), and true negatives (TN).\n",
    "\n",
    "By analyzing these cells, you can understand the types of errors your model is making:\n",
    "* If you have a high number of false positives (FP), it means that your model is incorrectly classifying negative cases as positive. This type of error is known as a Type I error or a false positive error.\n",
    "* If you have a high number of false negatives (FN), it means that your model is incorrectly classifying positive cases as negative. This type of error is known as a Type II error or a false negative error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9d9962",
   "metadata": {},
   "source": [
    "# Q8. What are some common metrics that can be derived from a confusion matrix, and how are they calculated?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d33908",
   "metadata": {},
   "source": [
    " There are several common metrics that can be derived from a confusion matrix:\n",
    "\n",
    "* **Accuracy**: Accuracy measures the overall correctness of the model's predictions. It is calculated as \n",
    "\n",
    "$\n",
    "\\frac{{TP + TN}}{{TP + TN + FP + FN}}\n",
    "$\n",
    "\n",
    "- Where :\n",
    "    - TP is the number of true positives\n",
    "    - TN is the number of true negatives \n",
    "    - FP is the number of false positives\n",
    "    - FN is the number of false negatives.\n",
    "\n",
    "* **Precision**: Precision measures the proportion of correctly predicted positive cases out of all positive predictions. It is calculated as\n",
    "\n",
    "$\n",
    "\\frac{{TP}}{{TP + FP}}\n",
    "$\n",
    "\n",
    "* **Recall**: Recall measures the proportion of correctly predicted positive cases out of all actual positive cases. It is calculated as \n",
    "\n",
    "$\n",
    "\\frac{{TP}}{{TP + FN}}\n",
    "$\n",
    "\n",
    "* **Specificity**: Specificity measures the proportion of correctly predicted negative cases out of all actual negative cases. It is calculated as\n",
    "\n",
    "$\n",
    "\\frac{{TN}}{{TN + FP}}\n",
    "$\n",
    "\n",
    "\n",
    "* **F1 Score**: The F1 score is the harmonic mean of precision and recall, providing a balanced measure between the two metrics. It is calculated as\n",
    "\n",
    "$\n",
    "\\frac{2 \\cdot (Precision \\cdot Recall)}{Precision + Recall}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba51a112",
   "metadata": {},
   "source": [
    "# Q9. What is the relationship between the accuracy of a model and the values in its confusion matrix?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e8b5b67",
   "metadata": {},
   "source": [
    "The accuracy of a model is related to the values in its confusion matrix as it is one of the metrics calculated using the values from the confusion matrix. The confusion matrix provides a detailed breakdown of the model's predictions and actual class labels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f93170a",
   "metadata": {},
   "source": [
    "# Q10. How can you use a confusion matrix to identify potential biases or limitations in your machine learning model?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63133f4c",
   "metadata": {},
   "source": [
    "\n",
    "To use a confusion matrix to identify potential biases or limitations in your machine learning model, you can look for the following patterns:\n",
    "\n",
    "* **Uneven distribution of true positives and true negatives**: This can indicate that the model is biased towards one class or another. For example, if the model has a high number of true positives for the positive class and a low number of true positives for the negative class, then this suggests that the model is more likely to predict positive examples, even if they are actually negative.\n",
    "* **High number of false positives or false negatives**: This can indicate that the model is not able to distinguish between the two classes very well. For example, if the model has a high number of false positives for the positive class, then this suggests that the model is predicting positive examples when they are actually negative.\n",
    "* **Different performance for different subpopulations**: This can indicate that the model is biased against certain subgroups. For example, if the model performs well for one group of people but poorly for another group of people, then this suggests that the model is not able to learn from the data of all groups equally.\n",
    "\n",
    "To identify potential biases or limitations in your model, you can create a confusion matrix for each subgroup of interest. This will allow you to compare the performance of the model for different subgroups."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
