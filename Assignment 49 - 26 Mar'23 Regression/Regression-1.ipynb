{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b23499cd",
   "metadata": {},
   "source": [
    "# Q1. Explain the difference between simple linear regression and multiple linear regression. Provide an example of each.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4d1689",
   "metadata": {},
   "source": [
    "In simple linear regression, there is a single independent feature and one dependent feature. Conversely, in multiple linear regression, there are multiple independent features and one dependent feature.\n",
    "\n",
    "For instance, a classic example of simple linear regression is the linear relationship between weight and height, where an increase in height corresponds to an increase in weight. On the other hand, an example of multiple linear regression is the relationship between various factors such as size, number of bedrooms, number of rooms, type of bathroom, and the price of a house."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b2fafc",
   "metadata": {},
   "source": [
    "# Q2. Discuss the assumptions of linear regression. How can you check whether these assumptions hold in a given dataset?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b900e01a",
   "metadata": {},
   "source": [
    "Linear regression is a statistical method used to model the relationship between a dependent variable and one or more independent variables. However, there are certain assumptions that must be met for linear regression to be valid. These include:\n",
    "\n",
    "* **Linearity**: The relationship between the dependent variable and independent variables should be linear.\n",
    " \n",
    "* **Independence**: The observations should be independent of each other.\n",
    " \n",
    "* **Homoscedasticity**: The variance of the errors should be constant across all levels of the independent variables.\n",
    " \n",
    "* **Normality**: The errors should be normally distributed.\n",
    "\n",
    "* **No multicollinearity**: The independent variables should not be highly correlated with each other.\n",
    " \n",
    "* **No outliers**: There should not be any extreme values that can significantly influence the results.\n",
    "\n",
    "To check whether the assumptions of linear regression are met in a given dataset: \n",
    "\n",
    "* **Linearity**: You can plot the dependent variable against each independent variable to visually inspect whether the relationship is linear.\n",
    " \n",
    "* **Independence**: You can check for independence by examining the data collection process and ensuring that there is no systematic bias or dependence between observations.\n",
    " \n",
    "* **Homoscedasticity**: You can plot the residuals (the differences between the predicted and actual values) against the predicted values to check for constant variance. You can also use statistical tests such as the Breusch-Pagan or White test to formally test for homoscedasticity.\n",
    " \n",
    "* **Normality**:  You can plot a histogram or a QQ plot of the residuals to check for normality. You can also use statistical tests such as the Shapiro-Wilk or Kolmogorov-Smirnov test to formally test for normality.\n",
    "\n",
    "* **No multicollinearity**: You can calculate the correlation coefficients between each pair of independent variables to check for high correlation. You can also use variance inflation factors (VIF) to quantify the degree of multicollinearity.\n",
    " \n",
    "* **No outliers**:  You can plot the residuals against each independent variable to check for outliers. You can also use statistical tests such as Cook's distance or leverage values to identify influential observations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "636f5cfd",
   "metadata": {},
   "source": [
    "# Q3. How do you interpret the slope and intercept in a linear regression model? Provide an example using a real-world scenario.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de1dc939",
   "metadata": {},
   "source": [
    "In a linear regression model, the slope represents the change in the dependent variable for a one-unit increase in the independent variable, while the intercept represents the value of the dependent variable when the independent variable is zero.\n",
    "\n",
    "For example suppose we want to model the relationship between a person's years of experience (independent variable) and their salary (dependent variable) in a particular industry. We collect data from a sample of individuals in that industry and fit a linear regression model:\n",
    "\n",
    "Salary = 30000 + 5000 * Years of Experience\n",
    "\n",
    "In this model, the intercept is 30000, which means that a person with zero years of experience would have a starting salary of 30,000. The slope is 5000, which means that for every one-year increase in experience, we would expect the person's salary to increase by 5000.\n",
    "\n",
    "Therefore, we can interpret the slope and intercept as follows:\n",
    "\n",
    "Intercept: The intercept represents the starting salary of a person with zero years of experience. In practice, this value is meaningful because it sets a baseline for salary expectations.\n",
    "Slope: The slope represents the increase in salary for a one-year increase in experience. In this example, the slope is positive, which means that more experienced individuals tend to have higher salaries than less experienced individuals."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c93208",
   "metadata": {},
   "source": [
    "# Q4. Explain the concept of gradient descent. How is it used in machine learning?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9094327",
   "metadata": {},
   "source": [
    "Gradient descent is a numerical optimization algorithm used to minimize the cost function in machine learning. The cost function is a measure of how well the model fits the data, and the goal of gradient descent is to find the set of model parameters that minimize the cost function.\n",
    "\n",
    "The basic idea of gradient descent is to iteratively adjust the model parameters in the direction of steepest descent of the cost function. This is done by computing the gradient of the cost function with respect to the model parameters and taking a step in the opposite direction of the gradient.\n",
    "\n",
    "The gradient is a vector that points in the direction of maximum increase of the cost function, so taking a step in the opposite direction of the gradient will decrease the cost function. The size of the step is determined by a learning rate parameter, which controls how quickly the algorithm converges to the minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c094da89",
   "metadata": {},
   "source": [
    "# Q5. Describe the multiple linear regression model. How does it differ from simple linear regression?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2404712f",
   "metadata": {},
   "source": [
    "Multiple linear regression is a statistical method used to model the relationship between a dependent variable and multiple independent variables. The model can be expressed as:\n",
    "\n",
    "Y = β0 + β1X1 + β2X2 + ... + βpXp + ε\n",
    "\n",
    "where Y is the dependent variable, X1, X2, ..., Xp are the independent variables, β0 is the intercept, β1, β2, ..., βp are the regression coefficients, and ε is the error term.\n",
    "\n",
    "The main difference between multiple linear regression and simple linear regression is the number of independent variables. In simple linear regression, there is only one independent variable, while in multiple linear regression, there are two or more independent variables. As a result, multiple linear regression can capture more complex relationships between the dependent variable and independent variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76dd4dd3",
   "metadata": {},
   "source": [
    "# Q6. Explain the concept of multicollinearity in multiple linear regression. How can you detect and address this issue?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf8cc3e",
   "metadata": {},
   "source": [
    "Multicollinearity refers to the presence of highly correlated independent variables, which can lead to unreliable regression coefficients. \n",
    "\n",
    "In order to detect multicollinearity, you can calculate the correlation coefficients between all independent variables and remove one of the highly correlated features.\n",
    "\n",
    "Alternatively, you can use variance inflation factors (VIF) to quantify the degree of multicollinearity. VIF measures how much the variance of the estimated regression coefficient is increased due to multicollinearity. A VIF value of 1 indicates no multicollinearity, while a value greater than 1 suggests increasing levels of multicollinearity. Generally, a VIF value greater than 5 or 10 is considered high and may require further investigation or remediation. By detecting and addressing multicollinearity, you can improve the accuracy and reliability of your linear regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c3e67e",
   "metadata": {},
   "source": [
    "# Q7. Describe the polynomial regression model. How is it different from linear regression?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f79dd0",
   "metadata": {},
   "source": [
    "Polynomial regression is a statistical method used to model the relationship between a dependent variable and an independent variable using a polynomial function. The model can be expressed as:\n",
    "\n",
    "Y = β0 + β1X + β2X^2 + ... + βpX^p + ε\n",
    "\n",
    "where Y is the dependent variable, X is the independent variable, β0 is the intercept, β1, β2, ..., βp are the regression coefficients, p is the degree of the polynomial, and ε is the error term.\n",
    "\n",
    "The main difference between polynomial regression and linear regression is that polynomial regression can capture nonlinear relationships between the dependent variable and independent variable.\n",
    "\n",
    "Polynomial regression can be useful when the relationship between the dependent variable and independent variable is not well approximated by a straight line. For example, if the relationship is curved or has multiple peaks and valleys, polynomial regression can provide a more accurate model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67d2379",
   "metadata": {},
   "source": [
    "# Q8. What are the advantages and disadvantages of polynomial regression compared to linear regression? In what situations would you prefer to use polynomial regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255147f3",
   "metadata": {},
   "source": [
    "The advantages of polynomial regression compared to linear regression are:\n",
    "\n",
    "* **Can model nonlinear relationships**: Polynomial regression can model nonlinear relationships between the dependent and independent variables, while linear regression assumes a linear relationship.\n",
    "\n",
    "* **More flexible**: Polynomial regression is more flexible than linear regression because it can fit a wider range of curves.\n",
    "\n",
    "The disadvantages of polynomial regression compared to linear regression are:\n",
    "\n",
    "* **Can overfit the data**: Polynomial regression can overfit the data if the degree of the polynomial is too high, which can lead to poor generalization to new data.\n",
    "\n",
    "* **More complex**: Polynomial regression is more complex than linear regression because it involves higher-order terms, which can make it more difficult to interpret the model.\n",
    "\n",
    "In situations where the relationship between the dependent and independent variables is nonlinear, polynomial regression can be a better choice than linear regression. For example, if we are modeling the relationship between temperature and ice cream sales, we might expect a quadratic or cubic relationship because sales might increase up to a certain temperature and then decrease at higher temperatures. In this case, polynomial regression can capture this nonlinear relationship better than linear regression."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
