{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b1bb67d",
   "metadata": {},
   "source": [
    "# Q1. Explain the concept of R-squared in linear regression models. How is it calculated, and what does it represent?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e6955f2",
   "metadata": {},
   "source": [
    "R-squared is a statistical measure of how well the predicted values from a regression model fit the actual data points. It is calculated as the percentage of the variance in the dependent variable that is explained by the independent variable(s).\n",
    "\n",
    "The formula for R-squared is:\n",
    "\n",
    "$\n",
    "R^2 = 1 - \\frac{SSR}{SST}\n",
    "$\n",
    "\n",
    "where:\n",
    "\n",
    "* SSR is the sum of squared residuals, which is the difference between the actual values of the dependent variable and the predicted values from the regression model.\n",
    "* SST is the total sum of squares, which is the sum of the squared differences between the actual values of the dependent variable and its mean.\n",
    "\n",
    "An R-squared value of 1 indicates that the regression model perfectly fits the data, while an R-squared value of 0 indicates that the model does not fit the data at all. A higher R-squared value indicates a better fit between the model and the data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0797d6",
   "metadata": {},
   "source": [
    "# Q2. Define adjusted R-squared and explain how it differs from the regular R-squared. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e229c5",
   "metadata": {},
   "source": [
    "Adjusted R-squared is a measure of the goodness of fit of a regression model that takes into account the number of independent variables in the model. It is calculated as follows:\n",
    "\n",
    "$\n",
    "adjusted R^2 = 1 - k * \\frac{(1 - R^2)(n - 1)}{(n - k - 1)}\n",
    "$\n",
    "\n",
    "Where:\n",
    "\n",
    "* k is the number of independent variables in the model\n",
    "* $R^2$ is the sample R-squared \n",
    "* n is the number of data points\n",
    "\n",
    "The most vital difference between adjusted R-squared and R-squared is simply that adjusted R-squared considers and tests different independent variables against the model and R-squared does not."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d20c903",
   "metadata": {},
   "source": [
    "# Q3. When is it more appropriate to use adjusted R-squared?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bea9c0f",
   "metadata": {},
   "source": [
    "Using adjusted R-squared over R-squared may be favored because of its ability to make a more accurate view of the correlation between one variable and another. Adjusted R-squared does this by taking into account how many independent variables are added to a particular model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0ae08b",
   "metadata": {},
   "source": [
    "# Q4. What are RMSE, MSE, and MAE in the context of regression analysis? How are these metrics calculated, and what do they represent?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23219a42",
   "metadata": {},
   "source": [
    "RMSE, MSE, and MAE are all measures of the error between the predicted values from a regression model and the actual values. They are calculated as follows:\n",
    "\n",
    "#### Mean squared error (MSE):\n",
    "MSE is the average of the squared errors. It is calculated as follows:\n",
    "\n",
    "$\n",
    "MSE = \\frac{\\sum\\limits_i (y_i - \\hat{y}_i)^2}{n}\n",
    "$\n",
    "where:\n",
    "* $y_i$ is the actual value of the dependent variable for the $i$th data point\n",
    "*  $\\hat{y}_i$ is the predicted value of the dependent variable for the $i$th data point\n",
    "* n is the number of data points\n",
    "\n",
    "\n",
    "#### Root mean squared error (RMSE):\n",
    "RMSE is the square root of MSE. It is calculated as follows\n",
    "\n",
    "$\n",
    "RMSE = \\sqrt{MSE}\n",
    "$\n",
    "\n",
    "#### Mean absolute error (MAE):\n",
    "MAE is the average of the absolute errors. It is calculated as follows:\n",
    "\n",
    "$\n",
    "MAE = \\frac{\\sum\\limits_i |y_i - \\hat{y}_i|}{n}\n",
    "$\n",
    "\n",
    "where:\n",
    "* $y_i$ is the actual value of the dependent variable for the $i$th data point\n",
    "*  $\\hat{y}_i$ is the predicted value of the dependent variable for the $i$th data point\n",
    "* n is the number of data points\n",
    "* ∣x∣ represents the absolute value of x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f7b8281",
   "metadata": {},
   "source": [
    "# Q5. Discuss the advantages and disadvantages of using RMSE, MSE, and MAE as evaluation metrics in regression analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51df947b",
   "metadata": {},
   "source": [
    "| Evaluation metrics | Advantages | Disadvantages |\n",
    "| --- | --- | --- |\n",
    "| MSE | It is differentiable , It has one local and one global Minima | Not Robust to Outliers , It is not in the same unit as the dependent variable |\n",
    "| MAE | Robust to Outliers , It is in the same unit as the dependent variable | Converge usually takes times , Time consuming |\n",
    "| RMSE | Same unit as the dependent variable , Differentiable | Not robust to Outliers |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c5bc28",
   "metadata": {},
   "source": [
    "# Q6. Explain the concept of Lasso regularization. How does it differ from Ridge regularization, and when is it more appropriate to use?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0806860a",
   "metadata": {},
   "source": [
    "The primary goal of LASSO regression is to find a balance between model simplicity and accuracy. It achieves this by adding a penalty term to the traditional linear regression model, which encourages sparse solutions where some coefficients are forced to be exactly zero. This feature makes LASSO particularly useful for feature selection, as it can automatically identify and discard irrelevant or redundant variables.\n",
    "\n",
    "The main difference between Lasso regularization and Ridge regularization is that Lasso regularization can set some of the coefficients to zero, while Ridge regularization cannot. This makes Lasso regularization more effective at feature selection, which is the process of identifying the most important features for the model.\n",
    "\n",
    "Lasso regularization is more appropriate to use when you are confident that some of the features in the model are not important. For example, if you are trying to predict the price of a house, you may be confident that the color of the house is not an important feature. In this case, Lasso regularization can be used to remove the color of the house from the model, which will make the model more accurate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744290f6",
   "metadata": {},
   "source": [
    "# Q7. How do regularized linear models help to prevent overfitting in machine learning? Provide an example to illustrate "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47dcc96a",
   "metadata": {},
   "source": [
    "Regularized linear models help to prevent overfitting by adding a penalty to the cost function that discourages the model from becoming too complex.\n",
    "\n",
    "Regularized linear models help to prevent overfitting by shrinking the coefficients towards zero, which reduces the complexity of the model.\n",
    "\n",
    "For example, let's say we have a linear regression model with two features: x1 and x2. The model is trained on a dataset of 100 data points. The training data is noisy, and the model learns to fit the noise as well as the underlying relationship between x1 and x2. This results in a model that is overfit to the training data.\n",
    "\n",
    "We can regularize the model by adding a penalty to the sum of the squared coefficients. This will shrink the coefficients towards zero, which will reduce the complexity of the model. As a result, the model will be less likely to overfit the training data.\n",
    "\n",
    "In this example, we can use the following regularization term:\n",
    "\n",
    "$\n",
    "penalty = \\lambda \\sum\\limits_{i=1}^n (w_i)^2\n",
    "$\n",
    "\n",
    "where:\n",
    "\n",
    "* $\\lambda$ is the regularization parameter\n",
    "* $w_i$ is the coefficient for feature x_i\n",
    "* n is the number of features\n",
    "\n",
    "The value of $\\lambda$ controls the amount of regularization. A larger value of $\\lambda$ will shrink the coefficients more, which will reduce the complexity of the model.\n",
    "\n",
    "We can choose the value of $\\lambda$ by cross-validation. Cross-validation is a technique for evaluating the performance of a model on unseen data. We can use cross-validation to find the value of $\\lambda$ that gives the best performance on the unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af485db3",
   "metadata": {},
   "source": [
    "# Q8. Discuss the limitations of regularized linear models and explain why they may not always be the best choice for regression analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8901312f",
   "metadata": {},
   "source": [
    "Here are some of the limitations of regularized linear models:\n",
    "\n",
    "* **Can lead to underfitting**: Regularized linear models can lead to underfitting if the regularization parameter is too large. This is because the model will be too simple and will not be able to capture the underlying relationship between the features and the target variable.\n",
    "* **Can be computationally expensive**: Regularized linear models can be computationally expensive to train, especially when the number of features is large.\n",
    "\n",
    "Here are some reasons why regularized linear models may not always be the best choice for regression analysis:\n",
    "* If the number of features is small, then regularized linear models may not be able to capture the underlying relationship between the features and the target variable.\n",
    "* If the data is not noisy, then regularized linear models may not be necessary. A simple linear regression model may be sufficient to fit the data without overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df258be",
   "metadata": {},
   "source": [
    "# Q9. You are comparing the performance of two regression models using different evaluation metrics. Model A has an RMSE of 10, while Model B has an MAE of 8. Which model would you choose as the better performer, and why? Are there any limitations to your choice of metric?\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d88171",
   "metadata": {},
   "source": [
    "The difference between the two metrics is that RMSE squares the errors before averaging them, while MAE does not. This means that RMSE gives more weight to larger errors, making it more sensitive to outliers.\n",
    "\n",
    "In this case, Model B has a lower MAE of 8, indicating that the average magnitude of errors between predicted and actual values is smaller. This suggests that Model B is more accurate in its predictions, on average, than Model A."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1add7aff",
   "metadata": {},
   "source": [
    "# Q10. You are comparing the performance of two regularized linear models using different types of regularization. Model A uses Ridge regularization with a regularization parameter of 0.1, while Model B uses Lasso regularization with a regularization parameter of 0.5. Which model would you choose as the better performer, and why? Are there any trade-offs or limitations to your choice of regularization method?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1e151e",
   "metadata": {},
   "source": [
    "In general, Ridge regularization is a good choice if you want to avoid overfitting without sacrificing too much accuracy. Lasso regularization is a good choice if you want to do feature selection as well as prevent overfitting.\n",
    "\n",
    "In this case, it is difficult to say which model would be the better performer without knowing more about the data. If the data is not very noisy, then Model A may be a better choice because it will not shrink the coefficients as much. However, if the data is noisy, then Model B may be a better choice because it will be more robust to noise."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
